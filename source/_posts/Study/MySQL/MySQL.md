# MySQL摘抄

## 01 | 基础架构：一条SQL查询语句是如何执行的？

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404021545606.png" alt="img" style="zoom: 33%;" />

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。

### 连接器

第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：

```
mysql -h$ip -P$port -u$user -p
```

输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在 -p 后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。

连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

**如果用户名或密码不对，你就会收到一个"Access denied for user"的错误**，然后客户端程序结束执行。如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，**这个连接里面的权限判断逻辑，都将依赖于此时读到的权限**。

这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 **show processlist** 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“**Sleep**”的这一行，就表示现在系统里面有一个空闲连接。

![img](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404021551355.png)

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 **wait_timeout** 控制的，默认值是 **8 小时**。

如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： **Lost connection to MySQL server during query**。这时候如果**你要继续，就需要重连，然后再执行请求了。**

数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是**尽量使用长连接。**

但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致**内存占用太大**，被系统强行杀掉**（OOM）**，从现象看就是 **MySQL 异常重启**了。

**怎么解决这个问题呢？你可以考虑以下两种方案。**

**定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。**

**如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。**

### 查询缓存

MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 **key-value** 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。

如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果**查询命中缓存，MySQL 不需要执行后面的复杂操作，**就可以直接返回结果，这个效率会很高。

**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。**

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。**除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。**

好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 **query_cache_type** 设置成 **DEMAND**，这样对于**默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定**，像下面这个语句一样：

```
mysql> select SQL_CACHE * from T where ID=10；
```

**需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。**

### 分析器

分析器先会做“**词法分析**”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。

MySQL 从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。

做完了这些识别以后，就要做“**语法分析**”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。

如果你的语句不对，就会收到“**You have an error in your SQL syntax**”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。

一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。

### 优化器

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join：

```
mysql> select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;
```

既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。

也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。

这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

### 执行器

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的**权限**，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

```
mysql> select * from T where ID=10;
ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'
```

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；

调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。

执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。你会在数据库的慢查询日志中看到一个 **rows_examined** 的字段，表示这个语句执行过程中扫描了多少行。这个值就是**在执行器每次调用引擎获取数据行的时候累加的**。

在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此**引擎扫描行数跟 rows_examined 并不是完全相同的**。我们后面会专门有一篇文章来讲存储引擎的内部机制，里面会有详细的说明。

### 问题：

我给你留一个问题吧，如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

## 02 | 日志系统：一条SQL更新语句是如何执行的？

一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。

那么，一条更新语句的执行流程又是怎样的呢？

MySQL 可以恢复到半个月内任意一秒的状态，惊叹的同时，你是不是心中也会不免会好奇，这是怎样做到的呢？

我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键 ID 和一个整型字段 c：

```
mysql> create table T(ID int primary key, c int);
```

如果要将 ID=2 这一行的值加 1，SQL 语句就会这么写：

```
mysql> update T set c=c+1 where ID=2;
```

**查询语句的那一套流程，更新语句也是同样会走一遍。**

你执行语句前要先连接数据库，这是连接器的工作。

前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。

接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。

与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：**redo log（重做日志）**和 **binlog（归档日志）**。如果接触 MySQL，那这两个词肯定是绕不过的，我后面的内容里也会不断地和你强调。不过话说回来，redo log 和 binlog 在设计上有很多有意思的地方，这些设计思路也可以用到你自己的程序里。

### 重要的日志模块：redo log

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似**酒店掌柜粉板的思路**来提升更新效率。

而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 **WAL 技术**，**WAL 的全称是 Write-Ahead Logging**，它的关键点就是**先写日志，再写磁盘**，也就是先写粉板，等不忙的时候再写账本。

当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。

如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。

与此类似，**InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。**从头开始写，**写到末尾就又回到开头循环写**，如下面这个图所示。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404021652827.png" alt="img" style="zoom: 33%;" />

write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

**循环队列**

**有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。**

### 重要的日志模块：binlog

粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。

**我想你肯定会问，为什么会有两份日志呢？**

因为最开始 **MySQL 里并没有 InnoDB 引擎**。**MySQL 自带的引擎是 MyISAM**，但是 **MyISAM 没有 crash-safe 的能力**，**binlog 日志只能用于归档**。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。

**这两种日志有以下三点不同**：

redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

redo log 是物理日志，记录的是“**在某个数据页上做了什么修改**”；binlog 是逻辑日志，记录的是这个语句的**原始逻辑**，比如“给 ID=2 这一行的 c 字段加 1 ”。

redo log 是**循环写的**，空间固定会用完；binlog 是可以**追加写入**的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。

执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。

执行器生成这个操作的 binlog，并把 binlog 写入磁盘。执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404021659118.png" alt="img" style="zoom: 50%;" />

你可能注意到了，最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。

### 两阶段提交

为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？

前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

- 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
- 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

**为什么日志需要“两阶段提交”**。这里不妨用**反证法**来进行解释。

由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是**先写完 redo log 再写 binlog**，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？

**先写 redo log 后写 binlog。**假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。	

但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。

然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。

**先写 binlog 后写 redo log。**如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？

其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要**扩容的时候**，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。

**简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。**

**总结：**

今天，我介绍了 MySQL 里面最重要的两个日志，即物理日志 redo log 和逻辑日志 binlog。

redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

我还跟你介绍了与 MySQL 日志系统密切相关的“两阶段提交”。两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。

文章的最后，我给你留一个思考题吧。前面我说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

## 03 | 事务隔离：为什么你改了我还看不见？

提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转 100 块钱，而此时你的银行卡只有 100 块钱。

转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。

简单来说，**事务就是要保证一组数据库操作，要么全部成功，要么全部失败。**在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 **MySQL 原生的 MyISAM 引擎就不支持事务**，**这也是 MyISAM 被 InnoDB 取代的重要原因之一**。

### 隔离性与隔离级别

提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。

当数据库上有多个事务同时执行的时候，就可能出现**脏读（dirty read）（读写）、不可重复读（non-repeatable read）（两次读和一次写）、幻读（phantom read）（两次写和一次读）**的问题，为了解决这些问题，就有了“隔离级别”的概念。

在谈隔离级别之前，你首先要知道，**你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。**SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释：

读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。

读提交是指，**一个事务提交之后，它做的变更才会被其他事务看到。**

可重复读是指，**一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。**当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。

串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。

```
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404022213565.png" alt="img" style="zoom: 33%;" />

我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。

若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。

若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。

若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。

若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。

**在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。**

**我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。**

配置的方式是，将启动参数 **transaction-isolation 的值设置成 READ-COMMITTED**。你可以用 show variables 来查看当前的值。

```
mysql> show variables like 'transaction_isolation';

+-----------------------+----------------+

| Variable_name | Value |

+-----------------------+----------------+

| transaction_isolation | READ-COMMITTED |

+-----------------------+----------------+
```

总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。

我想你可能会问那什么时候需要“可重复读”的场景呢？

**我们来看一个数据校对逻辑的案例。假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。**

### 事务隔离的实现

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031646698.png" alt="img" style="zoom: 50%;" />

当前值是 4，但是**在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）**。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。

你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。

什么时候才不需要了呢？就是**当系统里没有比这个回滚日志更早的 read-view 的时候**。

基于上面的说明，我们来讨论一下为什么建议你**尽量不要使用长事务。**

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会**导致大量占用存储空间**。

在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。

除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。

### 事务的启动方式

如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。

MySQL 的事务启动方式有以下几种：

1.显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。

2.set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

因此，我会建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。

但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。

在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

你可以在 **information_schema 库的 innodb_trx 这个表中查询长事务**，比如下面这个语句，用于查找持续时间超过 60s 的事务。

```
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

这篇文章里面，我介绍了 MySQL 的事务隔离级别的现象和实现，根据实现原理分析了长事务存在的风险，以及如何用正确的方式避免长事务。希望我举的例子能够帮助你理解事务，并更好地使用 MySQL 的事务特性。

## 04 | 深入浅出索引（上）

提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。**比如某一个 SQL 查询比较慢，分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案。**但到底什么是索引，索引又是如何工作的呢？

数据库索引的内容比较多，我分成了上下两篇文章。索引是数据库系统里面最重要的概念之一，所以我希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识点，加深你对数据库索引的理解。

一句话简单来说，**索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。**一本 500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，**索引其实就是它的“目录”。**

### 索引的常见模型

索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了**索引模型**的概念。可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是**哈希表、有序数组和搜索树**。

下面我主要从使用的角度，为你简单分析一下这三种模型的区别。

#### 哈希表

哈希表是一种以**键 - 值（key-value）**存储数据的结构，我们只要输入待查找的键即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。

不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示：

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031732990.png" alt="img" style="zoom:50%;" />

图中，User2 和 User4 根据身份证号算出来的值都是 N，但没关系，后面还跟了一个链表。假设，这时候你要查 ID_card_n2 对应的名字是什么，处理步骤就是：首先，将 ID_card_n2 通过哈希函数算出 N；然后，按顺序遍历，找到 User2。

需要注意的是，图中四个 ID_card_n 的值并**不是递增的**，这样做的好处是增加新的 User 时速度会很快，**只需要往后追加。**但缺点是，因为不是有序的，所以**哈希索引做区间查询的速度是很慢的。**

你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。

所以，**哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。**

哈希 字典 key value 

集合

#### 集合和字典有啥联系区别

集合（Set）和字典（Dictionary）都是常见的数据结构，它们在编程中有着不同的用途和特点。

1. **集合（Set）**：
   - 集合是一种无序且不重复的数据结构，它由一组唯一的元素组成。
   - 集合中的元素没有顺序，因此不能通过索引来访问，也不能通过索引来修改元素。
   - 集合主要用于判断一个元素是否存在于集合中，以及对集合进行交集、并集、差集等操作。
   - 在大多数编程语言中，集合的实现通常是基于哈希表（Hash Table）或类似的数据结构。
2. **字典（Dictionary）**：
   - 字典是一种键值对（Key-Value Pair）的数据结构，它将键映射到值，每个键都必须是唯一的。
   - 字典中的键是无序的，但是键值对之间的顺序是固定的（根据插入顺序或其他实现方式）。
   - 字典通常用于存储和查找键值对数据，其中通过键来快速查找对应的值。
   - 字典在各种编程语言中都有广泛的应用，如Python中的字典、JavaScript中的对象等。

联系和区别：

- 联系：集合和字典都是用于存储数据的数据结构，它们都支持存储不同类型的数据，并且都具有快速查找的特性。
- 区别：
  - 集合中的元素是无序的且不重复的，而字典中的键是唯一的，但值可以重复。
  - 集合用于存储单个元素的集合，而字典用于存储键值对的集合。
  - 集合通常用于查找和处理一组唯一的元素，而字典用于查找和处理具有特定键的值。

综上所述，集合和字典都是在编程中常用的数据结构，但它们在数据组织和使用方面有着不同的特点和用途。

**区别：**
**数据存储方式：**

集合是一组无序的唯一元素的集合，其中每个元素都是唯一的，没有重复的元素。
字典是由键-值对组成的集合，其中每个键都是唯一的，但值可以重复。
索引：

集合中的元素不能通过索引访问，因为集合是无序的。
字典中的元素是通过唯一的键来访问的，可以通过键来检索对应的值。

**联系：**
**可变性：**

集合和字典都是可变的数据结构，可以通过添加、删除和更新元素来改变它们的内容。
哈希表实现：

集合和字典都是通过哈希表来实现的，这使得它们在查找、插入和删除元素时具有较高的效率。

**集合适用于以下场景：**

**去重**：当需要从一组元素中去除重复项时，集合是一个很方便的数据结构。
**集合操作**：集合提供了丰富的操作方法，如并集、交集、差集等，适用于对多个集合进行操作的场景。
**成员关系测试**：集合提供了高效的成员关系测试，可以快速判断一个元素是否存在于集合中。
**字典适用于以下场景：**

**键值对存储**：当需要将一组键与对应的值关联起来时，字典是一个理想的数据结构。
**快速查找**：通过键来查找对应的值是字典的主要特性，适用于需要快速检索数据的场景。
**缓存**：字典可以用作缓存数据的数据结构，通过键来快速访问缓存的值，提高程序的运行效率。

#### 有序数组

**而有序数组在等值查询和范围查询场景中的性能就都非常优秀。**还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031810025.png" alt="img" style="zoom: 50%;" />

这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查 ID_card_n2 对应的名字，用**二分法**就可以快速得到，这个时间复杂度是 **O(log(N))**。

同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的 User，可以**先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证号，退出循环。**

如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，**在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。**

所以，**有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。**

二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：

#### 搜索树

二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031958945.png" alt="img" style="zoom:50%;" />

二叉搜索树的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。

这样如果你要查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -> UserC -> UserF -> User2 这个路径得到。这个时间复杂度是 O(log(N))。

当然为了维持 **O(log(N)) 的查询复杂度**，你就需要保持这棵树是**平衡二叉树**。为了做这个保证，**更新的时间复杂度也是 O(log(N))**。

二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。树可以有二叉，也可以有多叉。**多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增**。

你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的。

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。

以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，**查找一个值最多只需要访问 3 次磁盘。**其实，树的**第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。**

N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。

不管是哈希还是有序数组，或者 N 叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM 树等数据结构也被用于引擎设计中，这里我就不再一一展开了。

你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。

截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理数据的核心概念之一，在分析问题的时候会经常用到。当你理解了索引的模型后，就会发现在分析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。

#### n叉树遍历

n又树的遍历方式有多种，常见的有深度优先遍历和广度优先遍历。因此，n又树遍历的时间复杂度确实存在多个正确答案。下面分别给出这两种遍历方式的时间复杂度证明。
1.深度优先遍历的时间复杂度 假设n叉树的节点数为N，深度为d。由于每个节点都需要被访问一次，因此时间复杂度为O(N)。另外，由于是通过递归或者栈实现遍历，因此空间复杂度也是O(d)。

**深度优先遍历（Depth-First Search，DFS）**：

- 深度优先遍历是一种通过递归或栈的方式来遍历树的算法，其基本思想是尽可能深地搜索树的分支，直到不能继续为止，然后回溯到上一个分支继续搜索。
- 时间复杂度：O(N)，因为每个节点都需要被访问一次，其中 N 是树的节点数。
- 空间复杂度：O(d)，其中 d 是树的深度。这是由于在最坏情况下，递归调用或栈的大小与树的深度成正比。

```
class Node:
    def __init__(self, val, children):
        self.val = val
        self.children = children

def dfs(root: 'Node'):
    if not root:
        return
    # 访问当前节点
    print(root.val)
    # 递归遍历子节点
    for child in root.children:
        dfs(child)
```

2.广度优先遍历的时间复杂度 假设n叉树的节点数为N，深度为d。由于需要遍历每个节点一次，时间复杂度为O(N)。另外，每次需要将当前节点的所有子节点加入队列中，因此队列的大小不会超过每层节点数的最大值，即2^d。因此空间复杂度是0(2^d).。**广度优先遍历（Breadth-First Search，BFS）**：

- 广度优先遍历是一种通过队列的方式来遍历树的算法，其基本思想是按层级顺序逐层遍历树的节点。
- 时间复杂度：O(N)，因为每个节点都需要被访问一次，其中 N 是树的节点数。
- 空间复杂度：O(W)，其中 W 是树的最大宽度（即每层节点数的最大值）。这是因为需要使用队列来存储每层的节点。

```
from collections import deque

class Node:
    def __init__(self, val, children):
        self.val = val
        self.children = children

def bfs(root: 'Node'):
    if not root:
        return
    queue = deque([root])
    while queue:
        node = queue.popleft()
        # 访问当前节点
        print(node.val)
        # 将当前节点的所有子节点加入队列中
        for child in node.children:
            queue.append(child)
```

在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，所以下面我就以 InnoDB 为例，和你分析一下其中的索引模型。

#### InnoDB 的索引模型

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 **B+ 树**中的。

每一个索引在 InnoDB 里面对应一棵 B+ 树。假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。

这个表的建表语句是：

```
mysql> create table T(
id int primary key, 
k int not null, 
name varchar(16),
index (k))engine=InnoDB;
```

表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404032118108.png" alt="img" style="zoom:50%;" />

从图中不难看出，根据叶子节点的内容，索引类型分为**主键索引和非主键索引。**

**主键索引的叶子节点存的是整行数据。**在 InnoDB 里，主键索引也被称为**聚簇索引**（clustered index）。

**非主键索引的叶子节点内容是主键的值。**

在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

根据上面的索引结构说明，我们来讨论一个问题：**基于主键索引和普通索引的查询有什么区别？**

如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；

如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。

**也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。**

#### 索引维护

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。

除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。

当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。

基于上面的索引维护过程说明，我们来讨论一个案例：

**你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。**当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： **NOT NULL PRIMARY KEY AUTO_INCREMENT。**

插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。

**也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。**

而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。

除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？

由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。

**显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。**

所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：

**只有一个索引；**

**该索引必须是唯一索引。**

你一定看出来了，这就是典型的 KV 场景。由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。

今天，我跟你分析了数据库引擎可用的数据结构，介绍了 InnoDB 采用的 B+ 树结构，以及为什么 InnoDB 要这么选择。B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。由于 InnoDB 是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小。但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。最后，我给你留下一个问题吧。对于上面例子中的 InnoDB 表 T，如果你要重建索引 k，你的两个 SQL 语句可以这么写：

```
alter table T drop index k;
alter table T add index(k);
```

如果你要重建主键索引，也可以这么写：

```
alter table T drop primary key;
alter table T add primary key(id);
```

我的问题是，对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？

#### B+树

B+树的搜索、插入和删除操作的时间复杂度都是**O(log n)**，其中n为B+树中存储的关键字总数。 由于B+树通常用于存储大量的数据，因此它具有较高的效率和较快的响应速度，是一种非常优秀的数据结构。

B+树是一种常见的数据结构，用于在磁盘上存储和管理大量的数据。它的时间复杂度主要涉及到搜索、插入和删除三个操作。

1. 搜索操作的时间复杂度为O(log n)，其中n为B+树中存储的关键字总数。搜索操作的过程类似于二分查找，它可以在较短的时间内找到所需的关键字。
2. 插入操作的时间复杂度也为O(log n)。在插入新的关键字时，B+树需要先进行搜索，然后在叶子节点上插入新的关键字。如果插入导致某个节点的关键字数超过了上限，则需要进行分裂操作，将该节点分成两个节点。分裂操作可能会导致树的高度增加，但是它不会影响搜索和删除操作的时间复杂度。
3. 删除操作的时间复杂度也为O(log n)。在删除关键字时，B+树需要先进行搜索，然后将关键字从叶子节点中删除。如果删除导致某个节点的关键字数低于了下限，则需要进行合并操作，将该节点与相邻节点合并。合并操作也可能会导致树的高度减小，但是它同样不会影响搜索和插入操作的时间复杂度。

综上所述，B+树的搜索、插入和删除操作的时间复杂度都是O(log n)，其中n为B+树中存储的关键字总数。由于B+树通常用于存储大量的数据，因此它具有较高的效率和较快的响应速度，是一种非常优秀的数据结构。

![image-20240403222329505](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404032223620.png)

**他们看起来好像都完全重合了**！

![image-20240403223016854](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404032230915.png)

稍微解释一下：m 为什么可以等于$2^x$

m在这里就是大于2的自然数，这句话其实就是问$2^x$能不能表示任意一个大于2的自然数。当然是可以的，因为$2^x$是一条大于0的连续曲线。

所以：在内存里，当元素一样，b+树在一个节点内也采用二分法查找元素（最快的方式）。b树和二叉树的时间复杂度都是O(logN)。

https://blog.csdn.net/yunduanyou/article/details/128233801

## 05 | 深入浅出索引（下）

在下面这个表 T 中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？

下面是这个表的初始化语句。

```
mysql> create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;

insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
```

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404041005151.png" alt="img" style="zoom:50%;" />

现在，我们一起来看看这条 SQL 查询语句的执行流程：

在 k 索引树上找到 k=3 的记录，取得 ID = 300；

再到 ID 索引树查到 ID=300 对应的 R3；

在 k 索引树取下一个值 k=5，取得 ID=500；

再回到 ID 索引树查到 ID=500 对应的 R4；

在 k 索引树取下一个值 k=6，不满足条件，循环结束。

在这个过程中，回到主键索引树搜索的过程，我们称为**回表**。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。

### 覆盖索引

**文章解释的不是很清楚，见参考例文**

如果执行的语句是 select **ID** from T where k between 3 and 5，这时只需要查 ID 的值，**而 ID 的值已经在 k 索引树上了**，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。

由于覆盖索引**可以减少树的搜索次数，显著提升查询性能**，所以使用覆盖索引是一个常用的性能优化手段。

注：

**创建一个索引，该索引包含查询中用到的所有字段，称为“覆盖索引”。**

**使用覆盖索引，MySQL 只需要通过索引就可以查找和返回查询所需要的数据，而不必在使用索引处理数据之后再进行回表操作。**

**覆盖索引可以一次性完成查询工作，有效减少IO，提高查询效率。**

**也就是说，查询的列和索引的列一致，可以使用覆盖索引，查询中应该尽量使用覆盖索引，减少使用 `select * from`。**

需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。

基于上面覆盖索引的说明，我们来讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？

假设这个市民表的定义是这样的：

```
CREATE TABLE `tuser` (
  `id` int(11) NOT NULL,
  `id_card` varchar(32) DEFAULT NULL,
  `name` varchar(32) DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `ismale` tinyint(1) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `id_card` (`id_card`),
  KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB
```

我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。

https://juejin.cn/post/6844903967365791752

https://segmentfault.com/a/1190000039131878

### 最左前缀原则

看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？

这里，我先和你说结论吧。**B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。**

为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404041022862.jpeg" alt="img" style="zoom:50%;" />

可以看到，索引项是按照索引定义里面出现的字段顺序排序的。

当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。

如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是"where name like ‘张 %’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。

可以看到，不只是索引的全部定义，**只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。**

基于上面对最左前缀索引的说明，我们来讨论一个问题：**在建立联合索引的时候，如何安排索引内的字段顺序。**

这里我们的评估标准是，**索引的复用能力。**因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，**如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。**

所以现在你知道了，这段开头的问题里，我们要为高频请求创建 (身份证号，姓名）这个联合索引，并用这个索引支持“根据身份证号查询地址”的需求。那么，**如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。**

**这时候，我们要考虑的原则就是空间了。**比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。

### 索引下推

**文章解释的不是很清楚，见参考例文**

**索引下推条件：**

- **只能用于`range`、 `ref`、 `eq_ref`、`ref_or_null`访问方法；**
- **只能用于`InnoDB`和 `MyISAM`存储引擎及其分区表；**
- **对`InnoDB`存储引擎来说，索引下推只适用于二级索引（也叫辅助索引）;**

**MySQL服务层负责SQL语法解析、生成执行计划等，并调用存储引擎层去执行数据的存储和检索。**

**`索引下推`的下推其实就是指将部分上层（服务层）负责的事情，交给了下层（引擎层）去处理。**

**我们来具体看一下，在没有使用ICP的情况下，MySQL的查询：**

- **存储引擎读取索引记录；**
- **根据索引中的主键值，定位并读取完整的行记录；**
- **存储引擎把记录交给`Server`层去检测该记录是否满足`WHERE`条件。**

**使用ICP的情况下，查询过程：**

- **存储引擎读取索引记录（不是完整的行记录）；**
- **判断`WHERE`条件部分能否用索引中的列来做检查，条件不满足，则处理下一行索引记录；**
- **条件满足，使用索引中的主键去定位并读取完整的行记录（就是所谓的回表）；**
- **存储引擎把记录交给`Server`层，`Server`层检测该记录是否满足`WHERE`条件的其余部分。**

**尽量减少回表次数**

上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能要问，那些不符合最左前缀的部分，会怎么样呢？

我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的：

```
mysql> select * from tuser where name like '张%' and age=10 and ismale=1;
```

你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。

然后呢？当然是判断其他条件是否满足。

在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。

而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。图 3 和图 4，是这两个过程的执行流程图。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404041036685.jpeg" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404041036427.jpeg" alt="img" style="zoom:50%;" />

在图 3 和 4 这两个图里面，每一个虚线箭头表示回表一次。图 3 中，在 (name,age) 索引里面我特意去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次。

图 4 跟图 3 的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。

小结今天这篇文章，我和你继续讨论了数据库索引的概念，包括了**覆盖索引、前缀索引、索引下推**。你可以看到，在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。我们在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。

https://www.cnblogs.com/three-fighter/p/15246577.html

## 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

MySQL 的锁。数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

根据加锁的范围，**MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。**

这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。

### 全局锁

顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据**更新**语句（数据的**增删改**）、数据**定义**语句（包括**建表、修改表结构**等）和**更新类事务**的提交语句。

**全局锁的典型使用场景是，做全库逻辑备份。**也就是把整库每个表都 select 出来存成文本。

以前有一种做法，是通过 FTWRL 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。

但是让整库都只读，听上去就很危险：

如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；

如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。

**看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。**

假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。

现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。

如果时间顺序上是先备份账户余额表 (u_account)，然后用户购买，然后备份用户课程表 (u_course)，会怎么样呢？你可以看一下这个图：

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404060943989.png" alt="img" style="zoom:50%;" />

可以看到，这个备份结果里，用户 A 的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户 A 就发现，自己赚了。

作为用户可别觉得这样可真好啊，你可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？

也就是说，不加锁的话，**备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。**

说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，对吧？

是的，就是在可重复读隔离级别下开启一个事务。

官方自带的逻辑备份工具是 **mysqldump**。当 **mysqldump 使用参数–single-transaction 的时候**，**导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。**

你一定在疑惑，有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如**，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。**这时，我们就需要使用 FTWRL 命令了。

所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。**这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。**

你也许会问，**既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因：**

一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。

二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。

但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁。

### 表级锁

MySQL 里面表级别的锁有两种：**一种是表锁，一种是元数据锁（meta data lock，MDL)。**

**表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，**也可以在客户端断开的时候自动释放。需要注意，**lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。**

举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

**在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。**

**另一类表级的锁是 MDL（metadata lock)。**MDL 不需要显式使用，**在访问一个表的时候会被自动加上。**MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

因此，在 MySQL 5.5 版本中引入了 MDL，**当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。**

**读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。**

**读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。**

虽然 MDL 锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，我经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了。

你肯定知道，给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表 t 是一个小表。

备注：这里的实验环境是 MySQL 5.6。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404060943203.jpeg" alt="img" style="zoom:50%;" />

我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。

之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。

如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。

你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

**基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段？**

首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。

但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？

这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。

MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。

```
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ... 
```

小结

今天，我跟你介绍了 MySQL 的全局锁和表级锁。全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用–single-transaction 参数，对应用会更友好。

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是：

要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；

要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。

MDL 会直到事务提交才释放，在做表结构变更的时候，**你一定要小心不要导致锁住线上查询和更新。**

## 07 | 行锁功过：怎么减少行锁对性能的影响？

在上一篇文章中，我跟你介绍了 MySQL 的全局锁和表级锁，今天我们就来讲讲 MySQL 的行锁。

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

我们今天就主要来聊聊 InnoDB 的行锁，以及如何通过减少锁冲突来提升业务并发度。

顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导致程序出现非预期行为，比如两阶段锁。

### 从两阶段锁说起

我先给你举个例子。在下面的操作序列中，事务 B 的 update 语句执行时会是什么现象呢？假设字段 id 是表 t 的主键。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404060943319.jpeg" alt="img" style="zoom:50%;" />

这个问题的结论取决于事务 A 在执行完两条 update 语句后，持有哪些锁，以及在什么时候释放。你可以验证一下：实际上事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行。

知道了这个答案，你一定知道了事务 A 持有的两个记录的行锁，都是在 commit 的时候才释放的。

**也就是说，在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**

知道了这个设定，对我们使用事务有什么帮助呢？**那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**我给你举个例子。

假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作：

1. 从顾客 A 账户余额中扣除电影票价；
2. 给影院 B 的账户余额增加这张电影票价；
3. 记录一条交易日志。

也就是说，要完成这个交易，我们需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？

试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。

好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是，这并没有完全解决你的困扰。

如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的 MySQL 就挂了。你登上服务器一看，CPU 消耗接近 100%，但整个数据库每秒就执行不到 100 个事务。这是什么原因呢？

这里，我就要说到死锁和死锁检测了。

### 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404060943023.jpeg" alt="img" style="zoom:50%;" />

这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

**一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。**

**另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。**

在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，**超时时间设置太短的话，会出现很多误伤。**

所以，正常情况下我们还是要采用第二种策略，即：**主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。**主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。

你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

那如果是我们上面说到的所有事务都要更新同一行的场景呢？

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要**消耗大量的 CPU 资源。**因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

根据上面的分析，我们来讨论一下，怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的 CPU 资源。

**一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。**但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。

**另一个思路是控制并发度。**根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。

**因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。**

可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？

你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。

小结

今天，我和你介绍了 MySQL 的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。

其中，我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的原则 / 我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。

但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事务量。

最后，我给你留下一个问题吧。如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：

第一种，直接执行 delete from T limit 10000;

第二种，在一个连接中循环执行 20 次 delete from T limit 500;

第三种，在 20 个连接中同时执行 delete from T limit 500。

你会选择哪一种方法呢？为什么呢？你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾和你讨论这个问题。感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。

## 08 | 事务到底是隔离的还是不隔离的？

我在第 3 篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。

但是，我在上一篇文章中，和你分享行锁的时候又提到，一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？

我给你举一个例子吧。下面是一个只有两行的表的初始化语句。

```
mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `k` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, k) values(1,1),(2,2);
```

<img src="https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404060942124.png" alt="img" style="zoom:50%;" />

这里，我们需要注意的是事务的启动时机。

**begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。**

**第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；**

**第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。**

还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是**默认 autocommit=1。**

在这个例子中，事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。事务 B 在更新了行之后查询 ; 事务 A 在一个只读事务中查询，并且时间顺序上是在事务 B 的查询之后。

这时，如果我告诉你事务 B 查到的 k 的值是 3，而事务 A 查到的 k 的值是 1，你是不是感觉有点晕呢？

所以，今天这篇文章，我其实就是想和你说明白这个问题，希望借由把这个疑惑解开的过程，能够帮助你对 InnoDB 的事务和锁有更进一步的理解。

**在 MySQL 里，有两个“视图”的概念：**

**一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。**

**另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。**

它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。

在第 3 篇文章《事务隔离：为什么你改了我还看不见？》中，我跟你解释过一遍 MVCC 的实现逻辑。今天为了说明查询和更新的区别，我换一个方式来说明，把 read view 拆开。你可以结合这两篇文章的说明来更深一步地理解 MVCC。

### “快照”在 MVCC 里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。

注意，这个快照是基于整库的。这时，你会说这看上去不太现实啊。如果一个库有 100G，那么我启动一个事务，MySQL 就要拷贝 100G 的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。

实际上，我们并不需要拷贝出这 100G 的数据。我们先来看看这个快照是怎么实现的。

**InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。**

**而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。**

也就是说，数据表中的一行记录，**其实可能有多个版本 (row)，每个版本有自己的 row trx_id。**

如图 2 所示，就是一个记录被多个事务连续更新后的状态。

<img src="https://static001.geekbang.org/resource/image/68/ed/68d08d277a6f7926a41cc5541d3dfced.png?wh=1142*856" alt="img" style="zoom:50%;" />

图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。

你可能会问，前面的文章不是说，语句更新会生成 undo log（回滚日志）吗？那么，undo log 在哪呢？

实际上，图 2 中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。

明白了多版本和 row trx_id 的概念后，我们再来想一下，InnoDB 是怎么定义那个“100G”的快照的。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

**因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。**

当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

**在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。**

数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。

**这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。**

而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

这个视图数组把所有的 row trx_id 分成了几种不同的情况。

<img src="https://static001.geekbang.org/resource/image/88/5e/882114aaf55861832b4270d44507695e.png?wh=1142*856" alt="img" style="zoom:50%;" />

这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：

**如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；**

**如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；**

**如果落在黄色部分，那就包括两种情况**

**a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；**

**b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。**

比如，对于图 2 中的数据来说，如果有一个事务，它的低水位是 18，那么当它访问这一行数据时，就会从 V4 通过 U3 计算出 V3，所以在它看来，这一行的值是 11。

你看，有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？因为之后的更新，生成的版本一定属于上面的 2 或者 3(a) 的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。

所以你现在知道了，InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。

接下来，我们继续看一下图 1 中的三个事务，分析下事务 A 的语句返回的结果，为什么是 k=1。

这里，我们不妨做如下假设：

1. 事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；
2. 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；
3. 三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。

这样，事务 A 的视图数组就是[99,100], 事务 B 的视图数组是[99,100,101], 事务 C 的视图数组是[99,100,101,102]。

为了简化分析，我先把其他干扰语句去掉，只画出跟事务 A 查询逻辑有关的操作：

<img src="https://static001.geekbang.org/resource/image/94/49/9416c310e406519b7460437cb0c5c149.png?wh=1142*856" alt="img" style="zoom:50%;" />

从图中可以看到，第一个有效更新是事务 C，把数据从 (1,1) 改成了 (1,2)。这时候，这个数据的最新版本的 row trx_id 是 102，而 90 这个版本已经成为了历史版本。

第二个有效更新是事务 B，把数据从 (1,2) 改成了 (1,3)。这时候，这个数据的最新版本（即 row trx_id）是 101，而 102 又成为了历史版本。

你可能注意到了，在事务 A 查询的时候，其实事务 B 还没有提交，但是它生成的 (1,3) 这个版本已经变成当前版本了。但这个版本对事务 A 必须是不可见的，否则就变成脏读了。

好，现在事务 A 要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起的。所以，事务 A 查询语句的读数据流程是这样的：

找到 (1,3) 的时候，判断出 row trx_id=101，比高水位大，处于红色区域，不可见；

接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见；

再往前找，终于找到了（1,1)，它的 row trx_id=90，比低水位小，处于绿色区域，可见。

这样执行下来，虽然期间这一行数据被修改过，但是事务 A 不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。

这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，用于人肉分析可见性很麻烦。

所以，我来给你翻译一下。一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

版本未提交，不可见；

版本已提交，但是是在视图创建后提交的，不可见；

版本已提交，而且是在视图创建前提交的，可见。

现在，我们用这个规则来判断图 4 中的查询结果，事务 A 的查询语句的视图数组是在事务 A 启动的时候生成的，这时候：

(1,3) 还没提交，属于情况 1，不可见；

(1,2) 虽然提交了，但是是在视图数组创建之后提交的，属于情况 2，不可见；

(1,1) 是在视图数组创建之前提交的，可见。

你看，去掉数字对比后，只用时间先后顺序来判断，分析起来是不是轻松多了。所以，后面我们就都用这个规则来分析。

### 更新逻辑

细心的同学可能有疑问了：事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？

你看图 5 中，事务 B 的视图数组是先生成的，之后事务 C 才提交，不是应该看不见 (1,2) 吗，怎么能算出 (1,3) 来？

<img src="https://static001.geekbang.org/resource/image/86/9f/86ad7e8abe7bf16505b97718d8ac149f.png?wh=1142*856" alt="img" style="zoom:50%;" />

是的，如果事务 B 在更新之前查询一次数据，这个查询返回的 k 的值确实是 1。

但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务 C 的更新就丢失了。因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。

所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。

因此，在更新的时候，当前读拿到的数据是 (1,2)，更新后生成了新版本的数据 (1,3)，这个新版本的 row trx_id 是 101。

所以，在执行事务 B 查询语句的时候，一看自己的版本号是 101，最新数据的版本号也是 101，是自己的更新，可以直接使用，所以查询得到的 k 的值是 3。

这里我们提到了一个概念，叫作当前读。其实，除了 update 语句外，select 语句如果加锁，也是当前读。

所以，如果把事务 A 的查询语句 select * from t where id=1 修改一下，加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。

```
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```

再往前一步，假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？

<img src="https://static001.geekbang.org/resource/image/cd/6e/cda2a0d7decb61e59dddc83ac51efb6e.png?wh=906*565" alt="img" style="zoom:50%;" />

事务 C’的不同是，更新后并没有马上提交，在它提交前，事务 B 的更新语句先发起了。前面说过了，虽然事务 C’还没提交，但是 (1,2) 这个版本也已经生成了，并且是当前的最新版本。那么，事务 B 的更新语句会怎么处理呢？

这时候，我们在上一篇文章中提到的“两阶段锁协议”就要上场了。事务 C’没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务 B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’释放这个锁，才能继续它的当前读。

<img src="https://static001.geekbang.org/resource/image/54/92/540967ea905e8b63630e496786d84c92.png?wh=1142*856" alt="img" style="zoom:50%;" />

到这里，我们把一致性读、当前读和行锁就串起来了。

现在，我们再回到文章开头的问题：事务的可重复读的能力是怎么实现的？

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；

在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

那么，我们再看一下，在读提交隔离级别下，事务 A 和事务 B 的查询语句查到的 k，分别应该是多少呢？

这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的 start transaction。

下面是读提交时的状态图，可以看到这两个查询语句的创建视图数组的时机发生了变化，就是图中的 read view 框。（注意：这里，我们用的还是事务 C 的逻辑直接提交，而不是事务 C’）

<img src="https://static001.geekbang.org/resource/image/18/be/18fd5179b38c8c3804b313c3582cd1be.jpg?wh=1142*856" alt="img" style="zoom:50%;" />

这时，事务 A 的查询语句的视图数组是在执行这个语句的时候创建的，时序上 (1,2)、(1,3) 的生成时间都在创建这个视图数组的时刻之前。但是，在这个时刻：

(1,3) 还没提交，属于情况 1，不可见；

(1,2) 提交了，属于情况 3，可见。

所以，这时候事务 A 查询语句返回的是 k=2。显然地，事务 B 查询结果 k=3。

小结

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。

对于可重复读，查询只承认在事务启动前就已经提交完成的数据；

对于读提交，查询只承认在语句启动前就已经提交完成的数据；

而当前读，总是读取已经提交完成的最新版本。

你也可以想一下，为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。

当然，MySQL 8.0 已经可以把表结构放在 InnoDB 字典里了，也许以后会支持表结构的可重复读。

又到思考题时间了。我用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，我要把所有“字段 c 和 id 值相等的行”的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。

```
mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, c) values(1,1),(2,2),(3,3),(4,4);
```

<img src="https://static001.geekbang.org/resource/image/9b/0b/9b8fe7cf88c9ba40dc12e93e36c3060b.png?wh=486*661" alt="img" style="zoom:50%;" />



##  一文读懂，软件测试必会的方法和技术知识点！

软件测试是软件开发过程的重要组成部分，用来确认一个程序的品质或性能是否符合开发之前所提出的一些要求。软件测试人员要寻找Bug，避免软件开发过程中的缺陷，衡量软件的品质，关注用户的需求，总的目标确保软件的质量。 [测试面试宝典](https://cloud.tencent.com/developer/tools/blog-entry?target=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzk0NzAzNTM0Mg%3D%3D%26mid%3D100001972%26idx%3D1%26sn%3D5b6c7eb0423ae6f0459e7a1986942a0f%26chksm%3D437c4515740bcc03fc484c7911bc8385683056f0742664d572de2ff2df6c1b99a5e0abc4f25f%26scene%3D18%23wechat_redirect&source=article&objectId=1864380)

**一、按测试设计方法分类**

**1、黑盒测试**

黑盒测试是把测试对象看做一个黑盒子，利用黑盒测试法进行动态测试时，需要测试软件产品已经实现的功能是否符合功能设计要求，不需测试软件产品的内部结构和处理过程。

黑盒测试注重于测试软件的功能性需求，也即黑盒测试使软件工程师派生出执行程序所有功能需求的输入条件。黑盒测试并不是白盒测试的替代品，而是用于辅助白盒测试发现其他类型的错误。

**2、白盒测试**

设计者可以看到软件系统的内部结构，并使用软件的内部知识来指导测试数据及方法的选择。白盒测试通常被认为是单元测试与集成测试，期中有六种测试方法：语句覆盖、判定覆盖、条件覆盖、判定/条件覆盖、条件组合覆盖。

111

**3、灰盒测试**

介于黑盒和白盒之间是一种综合测试的方法，将白盒测试和黑盒测试结合在一起，构成一种无缝测试技术。灰盒测试是基于程序运行时的外部表现又结合程序内部逻辑结构来设计测试用例，执行程序并采集程序路径执行信息和外部用户接口结果的测试技术。灰盒测试法旨在验证软件满足外部指标以及软件的所有通道或路径都进行了检验。

实际工作中，对系统的了解越多越好。目前大多数的测试人员都是做黑盒测试，很少有做白盒测试的。因为白盒测试对软件测试人员的要求非常高需要有很多编程经验。做.NET程序的白盒测试你要能看得懂.NET代码。做JAVA程序的测试需要你能看懂JAVA代码。

**二、按测试是手动还是自动上分类**

**1、手动测试**　

测试人员用鼠标去手动测试，用鼠标各种点点点，手工测试更能容易发现软件的Bug。

**2、自动化测试**

用程序测试程序，由测试人员根据手工测试的Case来决定自动化测试的Case再编写程序或者脚本来替代手工做自动化测试。对于项目来说，手动测试和自动化测试同等重要，都是保障软件质量的方法。目前大部分的项目组都是手动测试和自动化测试相结合。因为很多测试无法做成自动化，很多复杂的业务逻辑也很难自动化，自动化测试无法取代手动测试。手工测试胜在测试业务逻辑，而自动化测试胜在测试底层架构。如果被测试的程序可测试性比较好很有必要做成自动化测试。

https://cloud.tencent.com/developer/article/1864380

## 产品规划蓝图：Roadmap长什么样子？

### 一、什么是Roadmap？

#### Roadmap的定义

Roadmap用来指引人们到达某个地点，或说明从甲地到达乙地的方式。从一般意义上来说，路线图更多的被指代于地理上的图片说明，例如寻宝图、部分位置图、交通示意图等等。

在广义上，Roadmap也可以是指引人们达到任何目标的说明性文档，甚至没有任何图片说明也可以被泛指为“Roadmap”。延伸到产品设计上，它是一种能快速表达出这款产品实现成功的过程和最终的结果。

#### Roadmap的特点

- **有起点有终点**

与传统地图不一样的地方是，Roadmap是有起点有终点。通常起点是1个，但是终点可能是1个或多个，因为产品发展的路径可能会从最开始的1个idea衍生成1个或多个产品解决方案。

- **有明确的路径**

这点和地图软件的导航很类似，从起点到终点的线路，一定是有明确的路径。基于用户不同诉求，路径是有所差异的，比如从A点到B点，按地铁优先、步行少、换乘少、时间短等不同的诉求，导航软件推荐的路径是不同的。但是不管是哪种路径，用户都能在地图上得到明确的路径，一步步指引自己从A点到达B点。

- **简单易懂、易于阅读**

Roadmap一般是浅显易懂的，按用户体验场景的流程，通过简洁、形象化的文字描述，来表达出来的图形。步骤清晰，且承载的信息相对较少，从而简单易懂、易于阅读。

### 二、为什么要做Roadmap？

- **完整性**

Roadmap可以从用户体验场景的横向和纵向两个维度，让团队直观的看到这款产品的完整面貌。

1. 横向看，从用户开始接触体验这款产品，到体验完成离开这款产品，整个用户体验路径上的场景都能被Roadmap覆盖。
2. 纵向看，在某一个用户体验场景上，比如从前期提供10%的该场景解决方案，到最终提供100%的该场景解决方案，都能在Roadmap上呈现出来。

- **成功的样子**

Roadmap的价值是让团队清晰的知道这款产品成功是什么样子？以及实现成功的样子需要设定哪些目标？这些目标要达到怎样的标准？这些标准需要在什么时候提供怎样的解决方案。这样在产品初期规划的时候，需要产品经理既要考虑用户场景、又要基于企业现有能力，最终描绘出一幅“可落地可执行成功产品的蓝图”。

- **能力储备**

一幅可落地可执行的Roadmap，可以让团队提前进行相应的能力储备，以便完美实现Roadmap成功的样子。

比如需要提前储备用户深度调研、需求挖掘、产品设计等能力，来解决用户在不同场景的痛点；

比如需要提前储备运营支撑能力，产品经理提供了产品解决方案，运营则需要匹配上合适的运营推广方案，从而更好的解决用户痛点；

再比如要实现对应的产品解决方案和运营推广方案，技术团队需要建设哪些技术能力？要不要建设中台？以及建设哪些中台？

### 三、Roadmap长什么样子？

![img](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031030676.webp)



Roadmap不在乎用什么工具来描绘，可以用PPT、Xmind等工具，但是它脱离不了以下这些核心的要素，通过这些核心要素按一定逻辑组合起来，呈现出的就是Roadmap的样子。

#### 目标

给你这张蓝图定一个一句话的目标，这句目标能完美的概括出这张蓝图的重点。比如C端O2O产品目标叫“围绕多、快、好、省来打造极致的用户在线交易及服务体验”；再比如B端CRM产品目标叫“建设客户全生命周期移动化、数字化管理平台”。让别人首先看到这句目标，就能快速的了解到这张蓝图的核心宗旨和愿景。

#### 横轴-完整用户体验路径

> 大多数**Roadmap的横轴都是用时间轴来呈现，但是我更偏向于用用户体验路径来当横轴。**
> 两者优劣点如下，大家可根据自己实际情况进行选择。

**横轴-时间**

- 优点：突出时间维度，能通盘清晰的看出每个时间段该做什么事
- 缺点：没有用户视角，缺乏横向逻辑性，比较难理解

**横轴-用户体验路径**

- 优点：站在用户视角，有横向逻辑性能串联整张**Roadmap**
- 缺点：不能通盘看到各个时间段该做什么事

将这款产品以用户体验路径拆解成几个阶段，再基于每个阶段识别出用户的核心动作，这样就形成横轴的用户体验路径。比如打车业务，可以分成上车前、车上、下车后3个阶段，每个阶段动作如下：

- 上车前，提交行程、等待接单、等待司机到达
- 车上，乘车
- 下车后，支付、评价、开票、投诉

将用户体验的关键阶段和动作识别出来，并基于此进行绘制每个阶段和动作场景下的Roadmap，这样就是一幅完美的Roadmap。

#### 纵轴-单个用户场景的蓝图

**1、用户行为**

用户行为主要是为了细化用户体验的关键阶段和动作。将动作拆解到具体的用户行为上。比如上车前的提交行程是一个动作，具体的用户行为是下载&打开App、注册&登录、填写行程（上车点、终点、用车时间等），然后才是提交行程。将这个动作下的用户行为进行拆解，一方面可以更加清晰的了解完整的用户行为，另一方面Roadmap也能细化到每个用户行为上。

通常可以根据“影响转化率”这个维度来拆解动作下的用户行为，比如影响提交行程的转化率有以下用户行为：

下载&打开滴滴出行App>注册&登录>选择出行服务>输入上车点>输入终点>确认用车时间>选择车型，然后完成订单提交

再将相同节点完成的用户行为进行合并，最终提交行程下的用户行为就是：下载&打开App、注册&登录、填写行程。

**2、接触点**

接触点主要指用户在上述动作过程中通过什么方式接触到什么样的人。这里的接触点可以分成2类：

- 系统接触点：指用户在上述动作接触到哪些系统的哪些核心功能
- 人员接触点：指用户在上述动作接触到哪些人员

比如等待司机这个动作下，

- 系统接触点：等待司机页（查看司机预计达到时间、车牌号等信息）
- 人员接触点：司机联系我、我联系客服

梳理关键阶段和动作下的接触点，是为了更好的了解到现有的功能支撑度和人员接触点。以系统和人员接触点为基础，构建对应动作下的Roadmap，最终提高该动作下的用户体验和转化率等目标。

**3、用户蓝图**

用户蓝图这个环节是Roadmap的核心环节，应该要包含以下要素：

- **蓝图内容：**

基于该动作下的用户痛点，输出对应的蓝图内容（即产品解决方案），可以是一条也可以是多条。输出的内容要能描述出该动作下“产品成功是长什么样的”。是站在用户视角下，该动作成功的样子。比如提交行程，对于用户来说，成功的样子就是要能快速提交行程，按用户行为可以拆解成3条蓝图内容：a、下载&打开要快；b、注册&登录要快；c、提交行程要快。

- **计划完成时间：**

有了每条蓝图内容，就要明确好在什么时间点能够完成上线。因为Roadmap通常是按年输出，所以计划完成时间精确到月份即可。评估计划完成时间，需要考虑到蓝图内容难易程度、实现成本、难易程度、人力成本，以及过程中涉及到的产品调研、输出方案、开发测试等环节总耗时情况，最终输出对应的计划完成时间，而不是拍脑袋写个时间点。

- **重要程度：**

将对应的蓝图内容按痛点程度进行拆解，通常越痛的重要程度越高。这就需要提前洞察用户痛点和识别痛点程度，基于此再进行重要程度标注。所以在描绘Roadmap之前，需先完成用户和业务调研，提前洞察用户所有痛点和痛点程度。

**4、衡量标准**

蓝图内容描绘的是“产品成功是长什么样的”，但是成没成功？成功到什么阶段？这些没法得知，所以需要一把”尺子“来衡量成功的标准。可以从定性和定量的两个维度，结合起来制定这把“尺子”。

- 定性标准：指不能直接量化而需通过其他途径实现量化的评估指标。比如将实现过程进行初步分级。
- 定量标准：可以准确数量定义、精确衡量并能设定绩效目标的考核指标。定量也是针对过程，和定性的区别是要有明确的数值。

比如转化率从10%->30%，10%指现状，30%指成功的标准。所以制定定量标准的时候，需要先计算当前标准，并且基于现状&未来蓝图，预测出成功的标准，这个预测标准要有依据，经得起推敲，而不是拍脑袋定的。

**5、业务支撑**

要完成蓝图内容，达到成功的样子，除了要有产品解决方案，还需要哪些业务支撑方案。即哪些业务部门需要参与哪些产品解决方案，便于更好的完成蓝图。不管是C端产品还是B端产品，要想实现蓝图内容，过程中都离不开业务部门进行支撑，比如前期一起制定业务规则、中期验收试点，后期上线推广，产品解决方案仅仅是蓝图的样子，如何将蓝图内容落地依赖业务支撑。

**6、系统支撑**

除了需要业务支撑，还少不了系统支撑。一个完整的C端App离开一套强大的后端系统支撑。B端的系统同样需要相关领域的系统支撑。主要以业务场景相关的领域系统支撑为主，故障监控、数据分析等增值系统支撑为辅。

### 四、设计Roadmap需要注意什么？

在设计和执行Roadmap过程中，应该关注以下3点：

#### 第1点：设定截止时间

这里的截止时间是指Roadmap是要在哪个时间点完成，是做1年的Roadmap，还是3年的Roadmap？1年的Roadmap是产品1年后成功的样子，3年的Roadmap是产品3年后成功的样子。不同的截止时间，Roadmap内容丰富和完整度是不同的。

所以，第1步需要先设定截止时间。

#### 第2点：完成前置条件

怎么理解Roadmap的前置条件？其实可以拆解Roadmap目标“让用户未来能体验这款成功样子的产品！”这目标里包含3个关键信息，一个是用户，一个是未来，最后才是成功的样子。达到这个目标，开始设计Roadmap前，则需要完成以下2个前置条件：

- **用户调研**：基于用户调研，内部了解现有业务场景，外部深度挖掘用户痛点和痒点，并按程度进行划分重要性。
- **业务规划**：培养业务敏感度，并提前了解中长期的业务规划内容，不贴合业务的Roadmap，就是一张不能落地的图画，对后续产品工作毫无指导意义。

#### 第3点：动态刷新

Roadmap不是一张静态图画，而是一张动态图画。随着时间推移，随着公司的战略变化，随着核心干系人变更等因素，都会影响到Roadmap内容。所以过程中遇到相关因素变化时，Roadmap应该随着变化而变化，要及时动态的刷新，并同步给团队的每个成员，而不是一尘不变。

### 五、总结

综上，一张**“好”**的Roadmap也需要遵循SMART原则。

> **SMART原则：**
> \1. 必须是具体的（Specific）
> \2. 必须是可以衡量的（Measurable）
> \3. 必须是可以达到的（Attainable）
> \4. 要与其他目标具有一定的相关性(Relevant)
> \5. 必须具有明确的截止期限（Time-bound）

大家看完本篇文章，都可以尝试针对自己的产品设计一张Roadmap。不用考虑自己有几年的产品经验？负责的产品规模多大？是不是负责核心模块？尝试设计一张Roadmap不仅是检验自己对产品的熟悉程度，也是培养自己用户思维和产品洞察力的过程。

## Nginx

### 一、Nginx是什么？

Nginx是lgor Sysoev为**俄罗斯访问量第二**的rambler.ru站点设计开发的。从2004年发布至今，凭借开源的力量，已经接近成熟与完善。

Nginx功能丰富，可作为**HTTP服务器，也可作为反向代理服务器**，邮件服务器。支持FastCGI、SSL、Virtual Host、URL Rewrite、Gzip等功能。并且支持很多第三方的模块扩展。

Nginx的稳定性、功能集、示例配置文件和低系统资源的消耗让他后来居上，在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。

Nginx (engine x) 是一个**高性能的HTTP和反向代理web服务器**，同时也提供了**IMAP/POP3/SMTP服务**。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、简单的配置文件和低系统资源的消耗而闻名。2011年6月1日，nginx 1.0.4发布。
Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，在BSD-like 协议下发行。其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。
Nginx 是高性能的 HTTP 和反向代理的web服务器，处理高并发能力是十分强大的，能经受高负 载的考验,有报告表明能支持高达 50,000 个并发连接数。
Nginx支持热部署，启动简单，可以做到7*24不间断运行。几个月都不需要重新启动。

### 二、Nginx的反向代理（扩展：正向代理）

![img](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031058050.png)

正向代理： 我们平时需要访问国外的浏览器是不是很慢，比如我们要看推特，看GitHub等等。我们直接用国内的服务器无法访问国外的服务器，或者是访问很慢。所以我们需要在本地搭建一个服务器来帮助我们去访问。那这种就是正向代理。（浏览器中配置代理服务器）

反向代理： 那什么是反向代理呢。比如：我们访问淘宝的时候，淘宝内部肯定不是只有一台服务器，它的内部有很多台服务器，那我们进行访问的时候，因为服务器中间session不共享，那我们是不是在服务器之间访问需要频繁登录，那这个时候淘宝搭建一个过渡服务器，对我们是没有任何影响的，我们是登录一次，但是访问所有，这种情况就是 反向代理。对我们来说，客户端对代理是无感知的，客户端不需要任何配置就可以访问，我们只需要把请求发送给反向代理服务器，由反向代理服务器去选择目标服务器获取数据后，再返回给客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器的地址。（在服务器中配置代理服务器）

### 三、Nginx的负载均衡

负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。
负载均衡（Load Balance）其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。

简单来说就是：现有的请求使服务器压力太大无法承受，所有我们需要搭建一个服务器集群，去分担原先一个服务器所承受的压力，那现在我们有ABCD等等多台服务器，我们需要把请求分给这些服务器，但是服务器可能大小也有自己的不同，所以怎么分？如何分配更好？又是一个问题。

Nginx给出来三种关于负载均衡的方式：

**轮询法（默认方法）**：
每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器宕掉，能自动剔除。
适合服务器配置相当，无状态且短平快的服务使用。也适用于图片服务器集群和纯静态页面服务器集群。
**weight权重模式（加权轮询）**：
指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。
这种方式比较灵活，当后端服务器性能存在差异的时候，通过配置权重，可以让服务器的性能得到充分发挥，有效利用资源。weight和访问比率成正比，用于后端服务器性能不均的情况。权重越高，在被访问的概率越大
**ip_hash**：
上述方式存在一个问题就是说，在负载均衡系统中，假如用户在某台服务器上登录了，那么该用户第二次请求的时候，因为我们是负载均衡系统，每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的。
我们可以采用ip_hash指令解决这个问题，如果客户已经访问了某个服务器，当用户再次访问时，会将该请求通过哈希算法，自动定位到该服务器。每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。

原文链接：https://blog.csdn.net/hyfsbxg/article/details/122322125

[Nginx 配置详解 | 菜鸟教程 (runoob.com)](https://www.runoob.com/w3cnote/nginx-setup-intro.html)

## CAD算法

https://www.bilibili.com/video/BV1dh411C7Pf

CAD（Computer-Aided Design，计算机辅助设计）是指利用计算机技术对产品进行设计、绘制、分析和优化的过程。CAD模型则是CAD软件生成的用于表示产品、零件或构件的三维模型。这些模型通常由各种几何形状、曲线、表面和体积组成，以精确地描述设计的对象。

CAD模型在工程设计、制造、建筑和其他领域中被广泛使用。它们可以用于创建产品的原型、进行工程分析、进行数字化加工和打印等。CAD模型可以通过CAD软件创建，如AutoCAD、SolidWorks、CATIA、Pro/ENGINEER等。

在CAD模型中，设计师可以添加尺寸、约束、材料属性和其他特性，以使模型更具实用性和可操作性。这些模型还可以与其他工程软件集成，以便进行进一步的分析、仿真和生产准备工作。

总的来说，CAD模型是一种重要的工具，它在现代工程设计和制造中起着至关重要的作用，为设计师提供了精确、可视化和易于操作的设计工具。

![image-20240403111126651](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031111729.png)

![image-20240403111655587](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202404031116664.png)

## 数值分析

https://www.bilibili.com/video/BV1sh41127RZ

https://www.bilibili.com/video/BV1iJ411473n

当涉及到数值计算方法时，这些算法是解决工程和科学领域中各种数学问题的常用工具。下面我将简要解释这些算法的基本原理：

1. **有限差分法（Finite Difference Method）**：
   - 有限差分法是一种数值计算方法，用于求解微分方程或积分方程的数值解。它通过将函数在空间上进行离散化，并将微分方程中的导数用差分来近似，从而将微分方程转化为代数方程组，进而求解。
   - 基本思想是将求解域划分为离散网格，并在每个网格点上近似原方程，然后通过差分近似导数，得到一个离散的代数方程系统，最终通过迭代或其他数值方法求解得到数值解。
2. **有限元法（Finite Element Method）**：
   - 有限元法是一种数值计算方法，用于求解偏微分方程的近似解。它将求解域划分为有限数量的简单几何形状单元，如三角形、四边形或六面体等，并在每个单元上近似原偏微分方程。
   - 基本思想是通过在每个单元上选择适当的试验函数来近似原方程，然后将所有单元的方程组合成一个整体方程组，通过求解这个方程组来得到数值解。
3. **线性方程组求解算法**：
   - 线性方程组求解算法是一种用于解决线性方程组的数值方法。常用的算法包括高斯消元法、LU分解、迭代法（如雅可比迭代法、高斯-赛德尔迭代法等）以及共轭梯度法等。
   - 这些算法旨在有效地求解形如Ax=b的线性方程组，其中A是一个已知的系数矩阵，b是一个已知的向量，而x是待求解的未知向量。
4. **矩阵分解算法**：
   - 矩阵分解算法是将一个矩阵分解为多个简单矩阵的方法，常用的分解包括LU分解、QR分解、奇异值分解（SVD）以及特征值分解等。
   - 这些分解可以帮助简化矩阵的运算，降低计算复杂度，同时在数值计算中有广泛的应用，如求解线性方程组、矩阵求逆、最小二乘拟合等。
5. **并行计算算法**：
   - 并行计算算法是一种利用多个计算资源同时工作来加速计算过程的方法。常用的并行计算技术包括并行编程模型（如MPI、OpenMP）、并行算法（如并行矩阵乘法、并行排序等）以及分布式计算等。
   - 这些算法和技术可用于加速大规模数据处理、复杂模拟和仿真等计算密集型任务，提高计算效率和性能。

这些算法都是数值计算领域中的基础工具，对于解决各种实际问题具有重要的作用。
