---
title: Prompt Learning
date: 2024-4-4
updated: 2024-04-04
categories:
- Prompt
tags:
- Study
description: 本篇博客记录prompt learning。
copyright: true
toc: true
---

本人很懒，不多介绍

<!-- more -->

#### 引言：

![image-20240326223806832](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403262238949.png)

具体

角色扮演

给予更多信息

#### 指南：

![image-20240327090826440](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270908575.png)

![image-20240327090915753](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270909811.png)

![image-20240327091018975](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270910024.png)2

![image-20240327100044644](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271000708.png)

##### 1.

![image-20240327091209446](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270912505.png)

![image-20240327094822023](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270948086.png)

使用分割号，避免提示词冲突

eg.```{text}```三引号，单引号，双引号、XML标记、章节标题等

##### 2.

![image-20240327095046678](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270950734.png)

##### 3.

![image-20240327095547850](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270955916.png)

![image-20240327095747126](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270957192.png)

![image-20240327095842598](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403270958665.png)

##### 4.

![image-20240327100600381](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271006438.png)

![image-20240327103543277](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271035342.png)

##### 1.

![image-20240327102423934](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271024016.png)

![image-20240327102451771](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271024841.png)

![image-20240327102539785](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271025840.png)

##### 2.

![image-20240327103012028](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271030089.png)

![image-20240327103044895](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271030957.png)

![image-20240327103122580](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271031644.png)

![image-20240327103138120](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271031165.png)

##### 3.

![image-20240327103237142](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271032190.png)

![image-20240327103257470](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271032549.png)

![image-20240327103329011](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271033071.png)

#### 迭代：

![image-20240327110040603](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271100684.png)

![image-20240327110121104](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271101168.png)

![image-20240327110320731](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271103809.png)

![image-20240327110341975](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271103041.png)

![image-20240327110432433](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271104495.png)

![image-20240327110517862](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271105920.png)

![image-20240327110542210](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271105251.png)

![image-20240327110600689](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271106742.png)

![image-20240327110705202](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271107273.png)

![image-20240327110857089](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271108138.png)

![image-20240327111048019](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271110077.png)

#### 摘要：

##### 1.

![image-20240327111204913](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271112964.png)

![image-20240327111217750](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271112799.png)

![image-20240327184724234](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271847303.png)

![image-20240327111300639](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271113701.png)

![image-20240327184829385](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271848443.png)

![image-20240327184848179](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271848273.png)

![image-20240327184905796](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271849884.png)

![image-20240327185317639](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271853708.png)

#### 推理：

##### 1.

![image-20240327185439935](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271854003.png)

![image-20240327185500551](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271855596.png)

![image-20240327185548248](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271855311.png)

![image-20240327185617511](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271856558.png)

![image-20240327185635539](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271856594.png)

![image-20240327185650704](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271856766.png)

![image-20240327185935837](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271859927.png)

![image-20240327190000677](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271900764.png)

![image-20240327190012497](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271900547.png)

![image-20240327190025567](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271900629.png)

![image-20240327190038489](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271900540.png)

![image-20240327190109214](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271901270.png)

![image-20240327190128847](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271901892.png)

#### 转换：

![image-20240327190245906](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271902968.png)

![image-20240327190301440](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271903487.png)

![image-20240327190337966](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271903018.png)

![image-20240327190418032](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271904089.png)

![image-20240327190432065](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271904151.png)

![image-20240327190452174](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271904236.png)

![image-20240327190507240](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271905301.png)

![image-20240327190532793](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271905835.png)

![image-20240327190603040](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271906089.png)

![image-20240327190547202](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271905251.png)

![image-20240327190626228](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271906290.png)

![image-20240327190732951](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271907033.png)

![image-20240327190740584](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271907648.png)

![image-20240327190759740](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271907798.png)

#### 扩展：

![image-20240327190834216](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271908324.png)

![image-20240327190848690](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271908750.png)

![image-20240327190905220](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271909284.png)

![image-20240327190925168](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271909235.png)

![image-20240327190943352](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271909424.png)

![image-20240327191052231](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271910305.png)

#### 聊天机器人：

![image-20240327191120487](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271911551.png)

![image-20240327191352772](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271913831.png)

![image-20240327191427813](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271914869.png)

![image-20240327191437774](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271914828.png)

![image-20240327191446800](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271914841.png)

![image-20240327191506565](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271915612.png)

![image-20240327191517669](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271915723.png)

![image-20240327191533989](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271915040.png)

![image-20240327191549796](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271915845.png)

![image-20240327191602349](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271916421.png)

![image-20240327191610985](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271916027.png)

![image-20240327191633952](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271916007.png)

![image-20240327191656489](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271916555.png)

![image-20240327191719657](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271917704.png)

![image-20240327191823715](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403271918774.png)

https://www.bilibili.com/video/BV1VZ421q7t3

### 课程总结：

![image-20240328165230690](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281652865.png)

关于这个prompt，我初步有两个想法：

一、用大模型LLM+prompt帮我生成指令。我前期的第五章工作用手写的代码计算，设置规则，生成指令。可以提供信息（物体类别、颜色、位置），设置prompt让大模型帮我尝试生成指令，最后做测试。

二、借鉴这篇论文的思路， 设计一套二维模型的流程，用现有的检测模型，例如轻量级的Yolo生成检测结果，以及空间计算信息，然后使用LLM设置prompt推理为目标中的哪一个

### 参考的论文demo结果：

```
How many doors are there in this room?
```

![image-20240328142346406](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281423592.png)

![image-20240328142325090](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281423187.png)

![image-20240328142431226](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281424320.png)

![image-20240328142509022](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281425124.png)

![image-20240328142538257](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281425380.png)

![image-20240328142552717](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281425820.png)

![image-20240328142605873](https://raw.githubusercontent.com/AngelYang23/Picture/master/Pic/202403281426977.png)



### 分析参考论文吗prompt：

### 角色目标：

```
SINGLE_TURN_MODE_SYSTEM_PROMPT = """You are a dialog agent that helps users to ground visual objects and answer questiosn in a 3D room scan using dialog. The user starts the conversation with some goal object in mind, your goals are:

1. Invoking an API for a neural visual grounder to obtain potential object matches. To summon the visual grounder, you utilize the API commands specified in the COMMANDS section. This API returns a JSON response containing potential target centroids, landmarks' centroids, their extents, and relationships between these elements. The API will also provide visual feedback indicating how the user's description correlates with images of the potential matches.
2. Examine the JSON results from the visual grounder, compare them against the user's description, and determine the appropriate object. More specifically: 
   2.1. Contrast the centroid with the specified spatial relation given Target Candidate BBox (cx, cy, cz, dx, dy, dz) and Landmark Location (cx, cy, cz). Note that positive z is the upward direction
   2.2. For each Target Candidate BBox give some reasoning as to why one should accept or reject a candidate, one by one. Choose and relay a unique ID that best matches these criteria. For example:

.candidate 1 should be accepted because ....; it should be rejected because...

.candidate 2 should be accepted because ....; it should be rejected because...
```

```
SINGLE_TURN_MODE_SYSTEM_PROMPT = """您是一个对话代理，可以使用对话帮助用户在 3D 房间扫描中定位可视对象并回答问题。用户带着某个目标对象开始对话，你的目标是
1. 调用神经视觉定位器的应用程序接口，获取潜在的匹配对象。要调用视觉定位器，您需要使用 "命令 "部分中指定的 API 命令。该应用程序接口会返回一个 JSON 响应，其中包含潜在目标中心点、地标中心点、其范围以及这些元素之间的关系。API 还将提供视觉反馈，显示用户的描述与潜在匹配图像之间的关联。
2.2. 检查视觉寻址器的 JSON 结果，将其与用户的描述进行比较，并确定合适的对象。更具体地说 
2.1. 根据给定的目标候选 BBox（cx、cy、cz、dx、dy、dz）和地标位置（cx、cy、cz），将中心点与指定的空间关系进行对比。注意正 z 是向上的方向
2.2. 对于每个目标候选 BBox，逐一给出接受或拒绝的理由。选择并转发一个最符合这些标准的唯一 ID。例如
- 候选人 1 应被接受，因为 ....；它应被拒绝，因为......
- 候选人 2 应被接受，因为 ....；它应被拒绝，因为......
```

### 1.

```
COMMANDS:
1. Visual grounder: "ground", args: "ground_json": "<ground_json>"
2. Finish grounding: "finish_grounding", args:  "top_5_objects_scores": "<top_5_objects_scores>", "top_1_object_id": "<top_1_object_id>"
You should only respond in JSON format as described below:

DETAILED DESCRIPTION OF COMMANDS:
1. Visual grounder: This command is termed "ground". Its arguments are: "ground_json": "<ground_json>". The ground json should be structured as follows:
Translate user descriptions into JSON-compatible format.
Highlight the principal object described as the "target", ensuring its uniqueness.
Recognize one most important auxiliary object mentioned only in relation to the main object and call it "landmark". Label the landmark as "landmark". Do not include generic objects such as "wall" or "floor" as the landmark. Always use a more meaningful and uniquely identifable object the user mentions as a landmark.


Example:
User Input: "I'm thinking of a white, glossy cabinet. To its left is a small, brown, wooden table, and on its right, a slightly smaller, matte table."

ground_json = {
    "target": {
        "phrase": "white, glossy cabinet"
    },
    "landmark": {
        "phrase": "small wooden brown table",
        "relation to target": "left"
    },
}

This output must be compatible with Python's json.loads() function. Once the grounder returns information, you must make a decision in the next response by mentioning which candidate to select in the next command. Note that for the target object, only include attributes and the noun.
```

```
指令：
1. Visual grounder： "ground", args: "ground_json"： "<ground_json>"
2. 完成接地 "finish_grounding", args: "top_5_objects_scores"： "<top_5_objects_scores>", "top_1_object_id"： "<top_1_object_id>"。
您只能以下面描述的 JSON 格式响应：

命令的详细说明：
1. 可视化接地： 该命令称为 "接地"。其参数如下 "ground_json"： "<ground_json>"。地面 json 的结构如下：
将用户描述转换为 JSON 兼容格式。
突出显示作为 "目标 "描述的主要对象，确保其唯一性。
识别一个仅与主要对象相关的最重要的辅助对象，并将其称为 "地标"。将地标标为 "地标"。不要将 "墙壁 "或 "地板 "等一般物体作为地标。一定要使用用户提到的更有意义且可唯一识别的物体作为地标。

示例：

用户输入 "我在想一个白色的光面柜子。左边是一张棕色的木质小桌子，右边是一张稍小的哑光桌子"。

ground_json = {
    "目标"： {
        "phrase"： "白色光面柜子"
    },
    "地标"： {
        "phrase"： "棕色木质小桌"、
        "与目标的关系"： "左"
    },
}

该输出必须与 Python 的 json.loads() 函数兼容。一旦 grounder 返回信息，您必须在下一个响应中做出决定，在下一条命令中提及选择哪个候选对象。请注意，对于目标对象，只需包含属性和名词。
```

### 2.

```
2. Finish Grounding: This command is termed "finish_grounding", with arguments: {"top_5_objects_scores": {"<object_id>": "<object_score>"}, "top_1_object_id": "<top_1_object_id>"}, where score is number between 0 and 1 that you need to decide based on all information to indicate how likely this object should be selected to match with the user query.

Example:

"args" = {
    "top_5_objects_scores": {"7": 0.93, "1": 0.81, "0": 0.67, "4": 0.51, "9": 0.44}
    "top_1_object_id": "7"
}

RESPONSE TEMPLATE:
{
    "thoughts":
    {
        "observation": "observation",
        "reasoning": "reasoning",
        "plan": "a numbered list of steps to take that conveys the long-term plan",
        "self-critique": "constructive self-critique",
        "speak": "thoughts summary to say to user"
    },
    "command": {
        "name": "command name",
        "args":{
            "arg name": "value"
        }
    }
}

Importantly, in your response JSON,  the "thoughts" section should be generated before the "command" section. Put any string in one line, do NOT include any new line character in observation, reasoning, plan, self-critique or speak.

Example:

{
"thoughts": {
"observation": "The user described a silver, cube-shaped file cabinet located in the middle of the northern side of the room. There is a whiteboard above it and a black couch to its right.",
"reasoning": "The user provided specific details about the file cabinet's shape, color, and location. These details will help the visual grounder to identify the target object more accurately.",
"plan": "1. Invoke the visual grounder with the translated ground_json. 2. Analyze the results from the visual grounder. 3. Choose the object that best matches the user's description based on the centroid and spatial relation.",
"self-critique": "The landmarks provided by the user are distinct and should aid in the accurate identification of the target object. However, if the visual grounder does not return a perfect match, further clarification may be required.",
"speak": "I am identifying the silver, cube-shaped file cabinet that is under a whiteboard and to the left of a black couch. It's located in the middle of the northern side of the room."
},
"command": {
"name": "ground",
"args": {
"ground_json": {
"target": {
"phrase": "silver cube shaped file cabinet"
},
"landmark": {
"phrase": "whiteboard",
"relation to target": "above"
},
}
}
}
}

Do not include generic objects such as "wall" or "floor" as the landmark. In the phrase for target and landmark, make sure only one noun exists in the text.
Again, your response should be in JSON format and can be parsed by Python json.loads().
"""
```

```
2. 完成接地： 该命令称为 "finish_grounding"，参数为 {"top_5_objects_scores": {"<object_id>"： "<对象_分数>"}, "top_1_object_id"： "<top_1_object_id>"}，其中 score 是介于 0 和 1 之间的数字，您需要根据所有信息来决定该对象被选中与用户查询匹配的可能性有多大。
例如

"args" = {
    "top_5_objects_scores"： {"7": 0.93, "1": 0.81, "0": 0.67, "4": 0.51, "9": 0.44}
    "top_1_object_id"： "7"
}

响应模板：
{
    "想法"：
    {
        "observation"： 观察
        "推理"： "推理"：
        "计划"： "一份编号的步骤清单，传达长期计划"、
        "自我批评 "建设性的自我批评
        "说"："对用户说的想法摘要"： "要对用户说的想法摘要"。
    },
    "命令"： {
        "名称"： "命令名称"、
        "args":{
            "arg name"： "值"
        }
    }
}
重要的是，在您的回复 JSON 中，"想法 "部分应在 "命令 "部分之前生成。在观察、推理、计划、自我批评或发言中，请勿包含任何新行字符。

例如

{
"想法"： {
观察 "用户描述了一个银色的立方体文件柜，位于房间的北侧中间。文件柜上方有一块白板，右侧有一张黑色沙发"、
"推理"： "用户提供了有关文件柜形状、颜色和位置的具体细节。这些细节将帮助视觉定位仪更准确地识别目标对象。"、
"计划"："1： "1. 使用翻译后的 ground_json 调用视觉 grounder。2. 2. 分析来自 visual grounder 的结果。3. 3. 根据中心点和空间关系选择最符合用户描述的对象"、
"自我批评"： "用户提供的地标很明显，应有助于准确识别目标对象。但是，如果视觉地标不能完全匹配，则可能需要进一步说明"、
"说"： "我正在识别白板下方、黑色沙发左侧的银色立方体文件柜。它位于房间北侧的中间位置。"
},
"命令"： {
"名称"： "ground"、
"args"： {
"ground_json"： {
"target"： {
"phrase"： "银色立方体文件柜"
},
"地标"： {
"phrase"： "白板"、
"与目标的关系"： "上方"
},
}
}
}
}

不要将 "墙壁 "或 "地板 "等一般物体作为地标。在目标和地标的短语中，确保文本中只存在一个名词。
同样，您的响应应该是 JSON 格式，可以用 Python json.loads() 解析。
"""
```

### 3.

```
3. Finish Grounding: This command is termed "finish_grounding", with arguments: {"top_5_objects_scores": {"<object_id>": "<object_score>"}, "top_1_object_id": "<top_1_object_id>"}, where score is number between 0 and 1 that you need to decide based on all information to indicate how likely this object should be selected to match with the user query.

Example:

"args" = {
    "top_5_objects_scores": {"7": 0.93, "1": 0.81, "0": 0.67, "4": 0.51, "9": 0.44}
    "top_1_object_id": "7"
}

RESPONSE TEMPLATE:
{
    "thoughts":
    {
        "observation": "observation",
        "reasoning": "reasoning",
        "plan": "a numbered list of steps to take that conveys the long-term plan",
        "self-critique": "constructive self-critique",
        "speak": "thoughts summary to say to user"
    },
    "command": {
        "name": "command name",
        "args":{
            "arg name": "value"
        }
    }
}

Importantly, in your response JSON,  the "thoughts" section should be generated before the "command" section. Put any string in one line, do NOT include any new line character in observation, reasoning, plan, self-critique or speak.

Example:

{
"thoughts": {
"observation": "The user described a silver, cube-shaped file cabinet located in the middle of the northern side of the room. There is a whiteboard above it and a black couch to its right.",
"reasoning": "The user provided specific details about the file cabinet's shape, color, and location. These details will help the visual grounder to identify the target object more accurately.",
"plan": "1. Invoke the visual grounder with the translated ground_json. 2. Analyze the results from the visual grounder. 3. Choose the object that best matches the user's description based on the centroid and spatial relation.",
"self-critique": "The landmarks provided by the user are distinct and should aid in the accurate identification of the target object. However, if the visual grounder does not return a perfect match, further clarification may be required.",
"speak": "I am identifying the silver, cube-shaped file cabinet that is under a whiteboard and to the left of a black couch. It's located in the middle of the northern side of the room."
},
"command": {
"name": "ground",
"args": {
"ground_json": {
"target": {
"phrase": "silver cube shaped file cabinet"
},
"landmark": {
"phrase": "whiteboard",
"relation to target": "above"
},
}
}
}
}

Do not include generic objects such as "wall" or "floor" as the landmark. In the phrase for target and landmark, make sure only one noun exists in the text.-【
Again, your response should be in JSON format and can be parsed by Python json.loads().
"""
```

```
3. 完成接地： 该命令称为 "finish_grounding"，参数为 {"top_5_objects_scores": {"<object_id>"： "<对象_分数>"}, "top_1_object_id"： "<top_1_object_id>"}，其中 score 是介于 0 和 1 之间的数字，您需要根据所有信息来决定该对象被选中与用户查询匹配的可能性有多大。

例如

"args" = {
    "top_5_objects_scores"： {"7": 0.93, "1": 0.81, "0": 0.67, "4": 0.51, "9": 0.44}
    "top_1_object_id"： "7"
}

响应模板：
{
    "想法"：
    {
        "observation"： "观察"、
        "推理"： "推理"：
        "计划"： "一份编号的步骤清单，传达长期计划"、
        "自我批评 "建设性的自我批评
        "说"："对用户说的想法摘要"： "要对用户说的想法摘要"。
    },
    "命令"： {
        "名称"： "命令名称"、
        "args":{
            "arg name"： "值"
        }
    }
}

重要的是，在您的回复 JSON 中，"想法 "部分应在 "命令 "部分之前生成。在观察、推理、计划、自我批评或发言中，将任何字符串放在一行中，不要包含任何新行字符。
```

### prompt：

```
# 模板一 <obj><location>
LOC_FIRST = {
    "left" : "on left, on the left side",
    "right" : "on right, on the right side",
    "front" : "in front",
    "back" : "in back",
    "top" : "on top, at top",
    "bottom" : "on bottom, at bottom",
    "top left" : "on top left, in top left, at top left",
    "bottom left" : "on bottom left, in bottom left, at bottom left",
    "top right" : "on top right, in top right, at top right",
    "bottom right" : "on bottom right, in bottom right, at bottom right",
    "front left" : "front left, in front left",
    "back left" : "back left, in the back left",
    "front right" : "front right, in front right",
    "back right" : "back right, in back right",
    "center" : "at center"
}
# 模板二 <location><obj> or the <location> of <obj> 
LOC_SECOND = {
    "left" : "left",
    "right" : "right",
    "top" : "top",
    "bottom" : "bottom",
    "top left" : "top left",
    "bottom left" : "bottom left",
    "top right" : "top right",
    "bottom right" : "bottom right",
    "front left" : "front left",
    "back left" : "back left",
    "front right" : "front right",
    "back right" : "back right",
}
# 模板三 <obj> <location> <obj_ref>
LOC_THIRD = {
    "left" : "to the left of",
    "right" : "to the right of",
    "front" : "in front of",
    "back" : "in back of",
    "top" : "at the top of, on the top of",
    "bottom" : "at the bottom of, on the bottom of",
    "top left" : "on top left of",
    "bottom left" : "on bottom left of",
    "top right" : "on top right of",
    "bottom right" : "on bottom right of",
    "front left" : "front left of",
    "back left" : "back left of",
    "front right" : "front right of",   
    "back right" : "back right of",
}
# 模板四 <order><obj><direction>  / <order><obj><location>
# equal to location first
ORDER_FOURTH = ["first, 1st", "second, 2nd", "third, 3th", "fourth, 4th", "fifth, 5th", "sixth, 6th", "seventh, 7th", "eighth, 8th", "ninth, 9th"]
# DIR_FOURTH = ["from left", "from right", "from top", "from bottom", "from front", "from back"]
DIR_FOURTH = ["from left", "from right", "from top", "from bottom"]
# 类别 id
ID_BBOX = {
    '0' : "other",
    '1' : "bottle",
    '2' : "bowl",
    '3' : "camera",
    '4' : "can",
    '5' : "laptop",
    '6' : "mug"
}
bbox:
rois = [(169, 435, 236, 499), (122, 85, 320, 338), (286, 52, 377, 171), (155, 322, 245, 416), (151, 522, 252, 577)]
ids:
ids = [6, 5, 2, 3, 4]

You are a agent that helps users to ground visual objects. your goals are:
1. given four types of template styles, or you can create your own colorful templates to describe objects and their spatial location relationships.
2. based on the templates, generate structured instruction templates that accurately describe the positional relationships between objects based on the input bbox (h1, w1, h2, w2) with the upper left corner as the coordinate origin representing (upper left vertical coordinate, upper left horizontal coordinate, lower right vertical coordinate, lower right horizontal coordinate) based on the input ids (the categories of the objects), using an appropriate strategy.

Example:
User Input: "I'm thinking of a black camera. To its left is a big, black laptop, and on its right, a slightly smaller mug."

ground_json = {
	"id":{
		"3"
	},
    "target": {
        "phrase_mode_1": "camera on right"
        "phrase_mode_2": "right camera"
        "phrase_mode_3": "camera to the right of laptop"
        "phrase_mode_4": "3th camera from left"
    },
}

This output must be compatible with Python's json.loads() function. Once the grounder returns information, you must make a decision in the next response by mentioning which candidate to select in the next command. Note that for the target object, only include local attributes and the noun.Don't code, just reason about the results.What I listed is just an example of one of the objects, can you generate the appropriate instructions based on the four templates?
```

### 1.

```
To achieve your goals, we can create functions to generate structured instruction templates based on the provided bbox and ids using the given template styles. Here's a Python implementation:

def generate_instructions(rois, ids):
    instructions = []

    for i, roi in enumerate(rois):
        obj_id = str(ids[i])
        obj_type = ID_BBOX.get(obj_id, "other")
        obj_location = get_object_location(roi)
        instructions.append((obj_type, obj_location))

    return instructions

def get_object_location(roi):
    h1, w1, h2, w2 = roi
    center_h = (h1 + h2) / 2
    center_w = (w1 + w2) / 2

    if center_h <= 0.25:
        if center_w <= 0.25:
            return "top left"
        elif center_w >= 0.75:
            return "top right"
        else:
            return "top"
    elif center_h >= 0.75:
        if center_w <= 0.25:
            return "bottom left"
        elif center_w >= 0.75:
            return "bottom right"
        else:
            return "bottom"
    else:
        if center_w <= 0.25:
            return "left"
        elif center_w >= 0.75:
            return "right"
        else:
            return "center"

def generate_template_one(obj_type, obj_location):
    return f"{obj_type} {LOC_FIRST[obj_location]}"

def generate_template_two(obj_type, obj_location):
    return f"{LOC_SECOND[obj_location]} {obj_type}"

def generate_template_three(obj_type, obj_location, obj_ref):
    return f"{obj_type} {LOC_THIRD[obj_location]} {obj_ref}"

def generate_template_four(order, obj_type, direction):
    return f"{ORDER_FOURTH[order]} {obj_type} {direction}"

rois = [(169, 435, 236, 499), (122, 85, 320, 338), (286, 52, 377, 171), (155, 322, 245, 416), (151, 522, 252, 577)]
ids = [6, 5, 2, 3, 4]

instructions = generate_instructions(rois, ids)

for i, (obj_type, obj_location) in enumerate(instructions):
    print(f"Object {i+1}:")
    print("Template One:", generate_template_one(obj_type, obj_location))
    print("Template Two:", generate_template_two(obj_type, obj_location))
    print("Template Three:", generate_template_three(obj_type, obj_location, "the object"))
    print("Template Four:", generate_template_four(i, obj_type, DIR_FOURTH[i]))
    print()

This script will generate structured instruction templates for each object based on its location within the image. You can adjust the logic in get_object_location function if you need different spatial divisions or use additional templates as needed.
```

### 2.

```
To achieve the goals, let's create a Python script to generate structured instruction templates based on the provided bbox and ids using the given template styles. Then, we'll format the output to be compatible with Python's json.loads() function. Here's the implementation:
import json

def generate_instructions(rois, ids):
    instructions = []

    for i, roi in enumerate(rois):
        obj_id = str(ids[i])
        obj_type = ID_BBOX.get(obj_id, "other")
        obj_location = get_object_location(roi)
        instructions.append((obj_type, obj_location))

    return instructions

def get_object_location(roi):
    h1, w1, h2, w2 = roi
    center_h = (h1 + h2) / 2
    center_w = (w1 + w2) / 2

    if center_h <= 0.25:
        if center_w <= 0.25:
            return "top left"
        elif center_w >= 0.75:
            return "top right"
        else:
            return "top"
    elif center_h >= 0.75:
        if center_w <= 0.25:
            return "bottom left"
        elif center_w >= 0.75:
            return "bottom right"
        else:
            return "bottom"
    else:
        if center_w <= 0.25:
            return "left"
        elif center_w >= 0.75:
            return "right"
        else:
            return "center"

def generate_template_one(obj_type, obj_location):
    return f"{obj_type} {LOC_FIRST[obj_location]}"

def generate_template_two(obj_type, obj_location):
    return f"{LOC_SECOND[obj_location]} {obj_type}"

def generate_template_three(obj_type, obj_location, obj_ref):
    return f"{obj_type} {LOC_THIRD[obj_location]} {obj_ref}"

def generate_template_four(order, obj_type, direction):
    return f"{ORDER_FOURTH[order]} {obj_type} {direction}"

def generate_ground_json(instructions):
    ground_json = {
        "id": {
            "3"
        },
        "target": {
            "phrase_mode_1": instructions[2][1],  # Using Template Two for simplicity
            "phrase_mode_2": instructions[2][1] + " " + instructions[0][0],  # Using Template Three for simplicity
            "phrase_mode_3": f"{instructions[2][0]} {LOC_THIRD[instructions[2][1]]} {instructions[0][0]}",
            "phrase_mode_4": f"{ORDER_FOURTH[2]} {instructions[0][0]} {DIR_FOURTH[1]}"
        }
    }
    return json.dumps(ground_json, indent=4)

# Sample Input
rois = [(169, 435, 236, 499), (122, 85, 320, 338), (286, 52, 377, 171), (155, 322, 245, 416), (151, 522, 252, 577)]
ids = [6, 5, 2, 3, 4]

# Generate Instructions
instructions = generate_instructions(rois, ids)

# Generate Ground JSON
ground_json = generate_ground_json(instructions)

print(ground_json)
This script generates the structured instruction templates and then constructs a JSON output compatible with Python's json.loads() function. Adjust the output format as needed to match your requirements.
```

### 3.

```
Based on the provided instructions and templates, here's how we can reason about the results:

Object Descriptions:

Object 1: Mug
Object 2: Laptop
Object 3: Bowl
Object 4: Camera
Object 5: Can
Spatial Relationships:

Mug (Object 1) is located towards the bottom right.
Laptop (Object 2) is located towards the top right.
Bowl (Object 3) is located towards the top left.
Camera (Object 4) is located towards the bottom left.
Can (Object 5) doesn't have a specified location, so it might be assumed to be somewhere in the center or not relevant to the described scene.
Template Selection:

For the target object (Camera), the phrase_mode_1 "camera on right" could be used, as it succinctly describes the object's position relative to the others.
Alternatively, phrase_mode_2 "right camera" could also be used for brevity.
For a more descriptive option, phrase_mode_3 "camera to the right of laptop" could be chosen.
Lastly, phrase_mode_4 "3th camera from left" could be selected if the order of objects matters, specifying the camera's position relative to others in the scene.
So, based on the provided information and templates, these are the reasoned selections for the target object's positional description.
```

### 4.

```
Based on the user input and the provided templates, here are the appropriate instructions for the target object (camera) based on the four templates:

Template One:

"camera on right"
Template Two:

"right camera"
Template Three:

"camera to the right of laptop"
Template Four:

"3rd camera from left"
These instructions describe the positional relationships of the camera relative to other objects in the scene using the specified templates.
```

### 5.

Sure, let's generate appropriate instructions based on all the objects and expand to create more colorful and diverse templates for each template style.

### Object Descriptions:

1. Mug (ID: 6)
2. Laptop (ID: 5)
3. Bowl (ID: 2)
4. Camera (ID: 3)
5. Can (ID: 4)

### Template One:

1. Mug (Object 1):
   - "mug at bottom right"
2. Laptop (Object 2):
   - "laptop at top right"
3. Bowl (Object 3):
   - "bowl at top left"
4. Camera (Object 4):
   - "camera at bottom left"
5. Can (Object 5):
   - "can at center"

### Template Two:

1. Mug (Object 1):
   - "bottom right mug"
2. Laptop (Object 2):
   - "top right laptop"
3. Bowl (Object 3):
   - "top left bowl"
4. Camera (Object 4):
   - "bottom left camera"
5. Can (Object 5):
   - "center can"

### Template Three:

1. Mug (Object 1):
   - "mug to the right of can"
2. Laptop (Object 2):
   - "laptop to the left of mug"
3. Bowl (Object 3):
   - "bowl in front of laptop"
4. Camera (Object 4):
   - "camera behind bowl"
5. Can (Object 5):
   - "can at the top of laptop"

### Template Four:

1. Mug (Object 1):
   - "1st mug from left"
2. Laptop (Object 2):
   - "2nd laptop from right"
3. Bowl (Object 3):
   - "3rd bowl from top"
4. Camera (Object 4):
   - "4th camera from bottom"
5. Can (Object 5):
   - "5th can from center"

These instructions describe the positional relationships of each object using all the four templates. They are diverse and colorful, providing various ways to describe the spatial relationships between the objects.